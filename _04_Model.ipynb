{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, LeakyReLU, Conv1D, GlobalAveragePooling1D, Flatten, MaxPooling1D,  BatchNormalization \n",
    "from tensorflow.keras.initializers import HeNormal, Constant\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import keras.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984caee",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6aec3",
   "metadata": {},
   "source": [
    "## divide intro Train, Validation and Test\n",
    "### Parameters:\n",
    "\n",
    "- **df**:  \n",
    "  The original DataFrame containing audio data.\n",
    "\n",
    "- **df_aug**:  \n",
    "  The augmented DataFrame, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of dialect labels for the training set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of features for the testing set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of dialect labels for the testing set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of dialect labels for the validation set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df, df_aug, name_aug, random_state=42):\n",
    "    \n",
    "    test_list = []\n",
    "    val_list = []\n",
    "    train_list = []\n",
    "\n",
    "    for cls in df['class'].unique():\n",
    "        df_cls = df[df['class'] == cls]\n",
    "        test_sample = df_cls.sample(frac=0.1, random_state=random_state)\n",
    "        remaining = df_cls.drop(test_sample.index)\n",
    "        val_sample = remaining.sample(frac=10/90, random_state=random_state)\n",
    "        train_sample = remaining.drop(val_sample.index)\n",
    "\n",
    "        test_list.append(test_sample)\n",
    "        val_list.append(val_sample)\n",
    "        train_list.append(train_sample)\n",
    "\n",
    "    test_df = pd.concat(test_list)\n",
    "    val_df = pd.concat(val_list)\n",
    "    train_df = pd.concat(train_list)\n",
    "\n",
    "    if df_aug is not None:\n",
    "        df_aug = df_aug.copy()\n",
    "        df_aug['base_file_name'] = df_aug['file_name'].apply(lambda x: x.split('_', 1)[1] if '_' in x else x)\n",
    "        eval_df = pd.concat([val_df, test_df]).copy()\n",
    "        eval_df['base_file_name'] = eval_df['file_name']\n",
    "        merge_keys = ['base_file_name', 'samples_begin', 'samples_end', 'class']\n",
    "        merged = pd.merge(df_aug, eval_df[merge_keys], on=merge_keys, how='left', indicator=True)\n",
    "        df_aug_filtered = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge', 'base_file_name'])\n",
    "        train_df = pd.concat([train_df, df_aug_filtered], ignore_index=True)\n",
    "\n",
    "    x_train = np.asarray(train_df['trillsson'].tolist())\n",
    "    y_train = train_df['class'].tolist()\n",
    "    x_val = np.asarray(val_df['trillsson'].tolist())\n",
    "    y_val = val_df['class'].tolist()\n",
    "    x_test = np.asarray(test_df['trillsson'].tolist())\n",
    "    y_test = test_df['class'].tolist()\n",
    "\n",
    "    y_test_names = test_df.file_name.tolist()\n",
    "    y_test_speaker = test_df.speaker.tolist()\n",
    "    y_test_segment_begin = test_df.samples_begin.tolist()\n",
    "    y_test_segment_end = test_df.samples_end.tolist()\n",
    "   \n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65080b1",
   "metadata": {},
   "source": [
    "## Divide into Train and Validation\n",
    "### Parameters:\n",
    "\n",
    "- **df**:  \n",
    "  The original DataFrame containing audio data.\n",
    "\n",
    "- **df_aug**:  \n",
    "  The augmented DataFrame, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of dialect labels for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of dialect labels for the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf57ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(df, df_aug, name_aug, random_state=42):\n",
    "\n",
    "    val_list = []\n",
    "    for cls in df['class'].unique():\n",
    "        df_cls = df[df['class'] == cls]\n",
    "        val_sample = df_cls.sample(frac=0.1, random_state=random_state)\n",
    "        val_list.append(val_sample)\n",
    "    val = pd.concat(val_list)\n",
    "    train = df.drop(val.index)\n",
    "\n",
    "    if df_aug is not None:\n",
    "        df_aug = df_aug.copy()\n",
    "        df_aug['base_file_name'] = df_aug['file_name'].apply(lambda x: x.split('_', 1)[1] if '_' in x else x)\n",
    "        val = val.copy()\n",
    "        val['base_file_name'] = val['file_name']\n",
    "        merge_keys = ['base_file_name', 'samples_begin', 'samples_end', 'class']\n",
    "        merged = pd.merge(df_aug, val[merge_keys], on=merge_keys, how='left', indicator=True)\n",
    "        df_aug_filtered = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge', 'base_file_name'])\n",
    "        train = pd.concat([train, df_aug_filtered], ignore_index=True)\n",
    "\n",
    "    x_train = np.asarray(train['trillsson'].tolist())\n",
    "    y_train = train['class'].tolist()\n",
    "    x_val = np.asarray(val['trillsson'].tolist())\n",
    "    y_val = val['class'].tolist()\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001fb6a7",
   "metadata": {},
   "source": [
    "## get Features\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of dialect labels for the training set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of features for the testing set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of dialect labels for the testing set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of dialect labels for the validation set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for encoding labels.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **y_train**:  \n",
    "  List of shuffled dialect labels for the training set.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of shuffled features for the training set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of shuffled dialect labels for the validation set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of shuffled features for the validation set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of shuffled dialect labels for the testing set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of shuffled features for the testing set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "\n",
    "- **yy_test**:  \n",
    "  Categorical labels for the testing set.\n",
    "\n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5980d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_learn):\n",
    "    y_train, x_train = shuffle(y_train, x_train, random_state=42)\n",
    "    y_val, x_val = shuffle(y_val, x_val, random_state=42+1)\n",
    "    y_test, x_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end = shuffle(y_test, x_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, random_state=42+2)\n",
    "\n",
    "    # Encode the classification labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(sorted(df_learn['class'].unique().tolist()))\n",
    "    yy_train = to_categorical(le.transform(y_train))    \n",
    "    yy_test = to_categorical(le.transform(y_test))\n",
    "    yy_val = to_categorical(le.transform(y_val))\n",
    "    \n",
    "    label_mapping = dict(zip(y_train, yy_train))\n",
    "    \n",
    "    return y_train, x_train, y_val, x_val, y_test, x_test, yy_train, yy_test, yy_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0294f",
   "metadata": {},
   "source": [
    "## get Features for Training and Validation\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of dialect labels for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of dialect labels for the validation set.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for encoding labels.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **y_train**:  \n",
    "  List of shuffled dialect labels for the training set.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of shuffled features for the training set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of shuffled dialect labels for the validation set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of shuffled features for the validation set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "\n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19fe1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_train_only(x_train, y_train, x_val, y_val, df_learn):\n",
    "    y_train, x_train = shuffle(y_train, x_train, random_state=42+3)\n",
    "    y_val, x_val = shuffle(y_val, x_val, random_state=42+4)\n",
    "\n",
    "    # Encode the classification labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(sorted(df_learn['class'].unique().tolist()))\n",
    "    yy_train = to_categorical(le.transform(y_train))\n",
    "    yy_val = to_categorical(le.transform(y_val))\n",
    "    \n",
    "    label_mapping = dict(zip(y_train, yy_train))\n",
    "    \n",
    "    return y_train, x_train, y_val, x_val, yy_train, yy_val, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81674b6d",
   "metadata": {},
   "source": [
    "## create model\n",
    "### Parameters:\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate for model optimization.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate for regularization.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter for the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter for the dense layers.\n",
    "\n",
    "- **alpha_val**:  \n",
    "  Alpha parameter for LeakyReLU.\n",
    "  \n",
    "- **finetune_only**:  \n",
    "  True if the run is only for finetuning.\n",
    "  \n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **model**:  \n",
    "  Compiled Keras model for classification.\n",
    "\n",
    "- **callback**:  \n",
    "  EarlyStopping callback to monitor validation loss and restore the best weights during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e6a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, alpha_val, finetune_only):\n",
    "    \n",
    "    seed_value = 42\n",
    "    he_initializer = HeNormal(seed=seed_value)\n",
    "    bias_initializer = Constant(value=0.0)\n",
    "    \n",
    "    classes = df_learn['class'].unique().tolist()\n",
    "    num_labels = len(classes)\n",
    "\n",
    "    METRICS = [\n",
    "        keras.metrics.TruePositives(name='tp'),\n",
    "        keras.metrics.FalsePositives(name='fp'),\n",
    "        keras.metrics.TrueNegatives(name='tn'),\n",
    "        keras.metrics.FalseNegatives(name='fn'), \n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.AUC(name='prc', curve='PR')\n",
    "    ]\n",
    "\n",
    "    def build_model_graph(metrics=METRICS):\n",
    "        model = Sequential()\n",
    "        model.add(BatchNormalization(input_shape=(np.array(x_train).shape[-1],), name='BatchNorm'))\n",
    "        model.add(Dense(units*2,\n",
    "                        kernel_regularizer=l2(l2_val), activity_regularizer=l1(l1_val),\n",
    "                        kernel_initializer=he_initializer, bias_initializer=bias_initializer, name='Dense1'))\n",
    "        model.add(LeakyReLU(alpha=alpha_val))\n",
    "        model.add(Dropout(dr, seed=seed_value, name='Dropout1'))\n",
    "\n",
    "        model.add(Dense(units,\n",
    "                        kernel_regularizer=l2(l2_val), activity_regularizer=l1(l1_val),\n",
    "                        kernel_initializer=he_initializer, bias_initializer=bias_initializer, name='Dense2'))\n",
    "        model.add(LeakyReLU(alpha=alpha_val))\n",
    "        model.add(Dropout(dr, seed=seed_value+1, name='Dropout2'))\n",
    "\n",
    "        model.add(Dense(num_labels, name='Output'))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "                      optimizer=Adam(learning_rate = lr))\n",
    "\n",
    "        if finetune_only:\n",
    "            model.load_weights('model_weights_train.h5')\n",
    "            dense2_index = model.layers.index(model.get_layer(\"Dense2\"))\n",
    "            for layer in model.layers[:dense2_index]:\n",
    "                layer.trainable = False\n",
    "\n",
    "        return model\n",
    "\n",
    "    model = build_model_graph()\n",
    "    print(model.summary())\n",
    "\n",
    "    callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode='min',\n",
    "        min_delta=0.005,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    return model, callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62041397",
   "metadata": {},
   "source": [
    "## train Model\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "\n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "\n",
    "- **class_weights**:  \n",
    "  Dictionary of class weights for handling class imbalance.\n",
    "\n",
    "- **model**:  \n",
    "  Compiled Keras model for training.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **num_epochs**:  \n",
    "  Number of epochs for training.\n",
    "\n",
    "- **callback**:  \n",
    "  EarlyStopping callback for monitoring validation loss.\n",
    "\n",
    "- **i**:  \n",
    "  Index used for TensorBoard log directory.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the model.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the model.\n",
    "  \n",
    "- **alpha_val**:  \n",
    "  Alpha parameter for LeakyReLU.\n",
    "\n",
    "- **tb**:  \n",
    "  Boolean indicating whether to enable TensorBoard logging.\n",
    "\n",
    "- **log_dir**:  \n",
    "  Directory path for TensorBoard logs.\n",
    "  \n",
    "- **train_only**:  \n",
    "  True if the run is only for training.\n",
    "  \n",
    "- **finetune_only**:  \n",
    "  True if the run is only for finetuning.\n",
    "\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **history**:  \n",
    "  History object containing training metrics and loss values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad8750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(y_train):\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    print('Class weights:', class_weights)\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "603170a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, num_epochs, callback, i, lr, dr, units, l1_val, l2_val, alpha_val, tb, log_dir, train_only, finetune_only):\n",
    "      \n",
    "    if tb:\n",
    "        log_dir = log_dir + str(i) + \"lr_\" + str(lr) + \"dr_\" + str(dr) + \"units_\" + str(units) + \"l1_\" + str(l1_val) + \"l2_\" + str(l2_val) + \"alpha_\" + str(alpha_val) + \"bs_\" + str(batch_size)\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "            log_dir=(log_dir), histogram_freq=1,\n",
    "        )\n",
    "        callbacks = [callback, tensorboard_callback]\n",
    "    else:\n",
    "        callbacks = [callback]\n",
    "    \n",
    "    history = model.fit(np.array(x_train), yy_train, batch_size=batch_size, epochs=num_epochs,\n",
    "                            validation_data=(np.array(x_val), yy_val), verbose=1,\n",
    "                            shuffle=False, class_weight=class_weights, callbacks=callbacks)\n",
    "    \n",
    "    if train_only:\n",
    "        model.save_weights('model_weights_train.h5')\n",
    "        \n",
    "    if finetune_only:\n",
    "        model.save_weights('model_weights_finetune.h5')\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82518459",
   "metadata": {},
   "source": [
    "## test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4f1a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x_test, yy_test, model):\n",
    "    pred_test = model.predict(np.array(x_test))\n",
    "        \n",
    "    classes_x=np.argmax(pred_test,axis=1)\n",
    "    classes_true=np.argmax(yy_test,axis=1)\n",
    "    df_result = pd.DataFrame(list(zip(classes_x, classes_true)), columns=['Pred', 'True'])\n",
    "    \n",
    "    return df_result, classes_x, classes_true, pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b9a90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_false(y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_result):\n",
    "    indices = df_result.index[df_result['Pred'] != df_result['True']].tolist()\n",
    "    false_names = [y_test_names[index] for index in indices]\n",
    "    false_speaker = [y_test_speaker[index] for index in indices]\n",
    "    false_segments_begin = [y_test_segment_begin[index] for index in indices]\n",
    "    false_segments_end = [y_test_segment_end[index] for index in indices]\n",
    "    false_simplified = [(y_test_names[index] + ' ' + str(y_test_segment_begin[index]/16000)) for index in indices]\n",
    "    return false_names, false_speaker, false_segments_begin, false_segments_end, false_simplified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af5f9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstPictures(history, num_epochs):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_loss, label='Training loss', color='#185fad')\n",
    "    plt.plot(val_loss, label='Validation loss', color='orange')\n",
    "    plt.title('Training and Validation loss by Epoch', fontsize = 25)\n",
    "    plt.xlabel('Epoch', fontsize = 18)\n",
    "    plt.xticks(range(0,num_epochs,5), range(0,num_epochs,5))\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig('ex_loss_epoch_.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    train_loss = history.history['accuracy']\n",
    "    val_loss = history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_loss, label='Training Accuracy', color='#185fad')\n",
    "    plt.plot(val_loss, label='Validation Accuracy', color='orange')\n",
    "    plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
    "    plt.xlabel('Epoch', fontsize = 18)\n",
    "    plt.xticks(range(0,num_epochs,5), range(0,num_epochs,5))\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig('ex_acc_epoch.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c6a2d",
   "metadata": {},
   "source": [
    "## main function\n",
    "### Parameters:\n",
    "\n",
    "- **first_pictures**:  \n",
    "  Boolean indicating whether to generate plots for the runs during training.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **df_learn_aug**:  \n",
    "  Augmented DataFrame containing audio data, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "- **i**:  \n",
    "  Index used for TensorBoard log directory.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the dense layers.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **tb**:  \n",
    "  Boolean indicating whether to enable TensorBoard logging.\n",
    "\n",
    "- **log_dir**:  \n",
    "  Directory path for TensorBoard logs.\n",
    "\n",
    "- **max_epochs**:  \n",
    "  Maximum number of epochs for training.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **list_row**:  \n",
    "  A list containing various metrics and data for evaluation and analysis.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n",
    "\n",
    "### Description:\n",
    "\n",
    "This function orchestrates the entire workflow for training and evaluating the classification model for dialect classification. It performs data preprocessing, model creation, training, evaluation, and result extraction. Depending on the parameters, it either trains a custom neural network model or a DummyClassifier. It returns a list of evaluation metrics and data for analysis, along with a mapping of original dialect labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eddc8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all(first_pictures, df_learn, df_learn_aug, name_aug, i, lr, dr, units, l1_val, l2_val, alpha_val, batch_size, tb, log_dir, max_epochs):\n",
    "\n",
    "    x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end = train_test(df_learn, df_learn_aug, name_aug)\n",
    "    y_train, x_train, y_val, x_val, y_test, x_test, yy_train, yy_test, yy_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, label_mapping = features(x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_learn)\n",
    "    class_weights = weights(y_train)\n",
    "    model, callback = create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, alpha_val, False)\n",
    "    history = train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, max_epochs, callback, i, lr, dr, units, l1_val, l2_val, alpha_val, tb, log_dir, False, False)\n",
    "    df_result, classes_x, classes_true, pred_test = pred(x_test, yy_test, model)\n",
    "    loss, accuracy = model.evaluate(np.array(x_test), yy_test, verbose=0)  \n",
    "    false_names, false_speaker, false_segments_begin, false_segments_end, false_simplified = get_false(y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_result)\n",
    "    if first_pictures:\n",
    "        firstPictures(history, max_epochs)\n",
    "    \n",
    "    list_row = [history.history['accuracy'][-1], history.history['val_accuracy'][-1], accuracy,\n",
    "                history.history['loss'][-1], history.history['val_loss'][-1], loss,\n",
    "                false_simplified, classes_x, classes_true, pred_test]\n",
    "    \n",
    "    return list_row, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02442627",
   "metadata": {},
   "source": [
    "## main function for Training\n",
    "### Parameters:\n",
    "\n",
    "- **model_num**\n",
    "  The Number of the Model for saving the weights\n",
    "\n",
    "- **first_pictures**:  \n",
    "  Boolean indicating whether to generate plots for the runs during training.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **df_learn_aug**:  \n",
    "  Augmented DataFrame containing audio data, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the dense layers.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **max_epochs**:  \n",
    "  Maximum number of epochs for training.\n",
    "  \n",
    "- **finetune_only**:  \n",
    "  True if the run is only for finetuning.\n",
    "  \n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **list_row**:  \n",
    "  A list containing various metrics and data for evaluation and analysis.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n",
    "\n",
    "### Description:\n",
    "\n",
    "This function orchestrates the entire workflow for training the classification model for dialect classification. It performs data preprocessing, model creation, training and saving the model weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78bd1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_train_finetune(first_pictures, df_learn, df_learn_aug, name_aug, lr, dr, units, l1_val, l2_val, alpha_val, batch_size, max_epochs, finetune_only):\n",
    "\n",
    "    x_train, y_train, x_val, y_val = train_val(df_learn, df_learn_aug, name_aug)\n",
    "    y_train, x_train, y_val, x_val, yy_train, yy_val, label_mapping = features_train_only(x_train, y_train, x_val, y_val, df_learn)\n",
    "    class_weights = weights(y_train)\n",
    "    model, callback = create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, alpha_val, finetune_only)\n",
    "    history = train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, max_epochs, callback, 0, lr, dr, units, l1_val, l2_val, alpha_val, False, '', not finetune_only, finetune_only)\n",
    "    if first_pictures:\n",
    "        firstPictures(history, max_epochs)\n",
    "    \n",
    "    list_row = [history.history['accuracy'][-1], history.history['val_accuracy'][-1], '',\n",
    "                history.history['loss'][-1], history.history['val_loss'][-1], '',\n",
    "                '', '', '', '']\n",
    "    \n",
    "    return list_row, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d22e4b",
   "metadata": {},
   "source": [
    "## main function for Testing\n",
    "### Parameters:\n",
    "\n",
    "- **model_num**\n",
    "  The Number of the Model for loading the right weights\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **df_learn_test**:  \n",
    "  DataFrame containing audio data used for model testing.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the dense layers.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **predictions**:  \n",
    "  A list containing all predictions.\n",
    "\n",
    "### Description:\n",
    "\n",
    "This function orchestrates the entire workflow for testing the classification model for dialect classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbd1641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_test(df_learn, df_test, lr, dr, units, l1_val, l2_val, alpha_val):\n",
    "    \n",
    "    x_test = np.asarray(df_test['trillsson'].tolist())\n",
    "    x_samples_begin = np.asarray(df_test['samples_begin'].tolist())\n",
    "    x_samples_end = np.asarray(df_test['samples_end'].tolist())\n",
    "    model, callback = create_model(df_learn, x_test, lr, dr, units, l1_val, l2_val, alpha_val, False)\n",
    "    model.load_weights('model_weights_finetune.h5')\n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "    return predictions, x_samples_begin, x_samples_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b503a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
