{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e1a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lea\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, LeakyReLU, Conv1D, GlobalAveragePooling1D, Flatten, MaxPooling1D,  BatchNormalization, LayerNormalization  \n",
    "from tensorflow.keras.initializers import HeNormal, Constant\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from keras.utils import to_categorical\n",
    "from scipy.optimize import minimize\n",
    "from keras.models import Sequential\n",
    "from betacal import BetaCalibration\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import keras.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984caee",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db49357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6aec3",
   "metadata": {},
   "source": [
    "## divide intro Train, Validation and Test\n",
    "### Parameters:\n",
    "\n",
    "- **df**:  \n",
    "  The original DataFrame containing audio data.\n",
    "\n",
    "- **df_aug**:  \n",
    "  The augmented DataFrame, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of labels for the training set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of features for the testing set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of labels for the testing set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of labels for the validation set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(df, df_aug, name_aug, random_state=42):\n",
    "    \n",
    "    test_list = []\n",
    "    val_list = []\n",
    "    train_list = []\n",
    "\n",
    "    for cls in df['class'].unique():\n",
    "        df_cls = df[df['class'] == cls]\n",
    "        test_sample = df_cls.sample(frac=0.1, random_state=random_state)\n",
    "        remaining = df_cls.drop(test_sample.index)\n",
    "        val_sample = remaining.sample(frac=10/90, random_state=random_state)\n",
    "        train_sample = remaining.drop(val_sample.index)\n",
    "\n",
    "        test_list.append(test_sample)\n",
    "        val_list.append(val_sample)\n",
    "        train_list.append(train_sample)\n",
    "\n",
    "    test_df = pd.concat(test_list)\n",
    "    val_df = pd.concat(val_list)\n",
    "    train_df = pd.concat(train_list)\n",
    "\n",
    "    if df_aug is not None:\n",
    "        train_df_aug_key = train_df.copy()\n",
    "        df_aug = df_aug.copy()\n",
    "        train_df_aug_key['base_file_name'] = train_df_aug_key['file_name']\n",
    "        df_aug['base_file_name'] = df_aug['file_name'].apply(lambda x: x.split('_', 1)[1] if '_' in x else x)\n",
    "        merge_keys = ['base_file_name', 'samples_begin', 'samples_end', 'class']\n",
    "        merged = pd.merge(df_aug, train_df_aug_key[merge_keys], on=merge_keys, how='inner')\n",
    "        train_df = pd.concat([train_df, merged.drop(columns=['base_file_name'])], ignore_index=True)\n",
    "\n",
    "    x_train = np.asarray(train_df['trillsson'].tolist())\n",
    "    y_train = train_df['class'].tolist()\n",
    "    x_val = np.asarray(val_df['trillsson'].tolist())\n",
    "    y_val = val_df['class'].tolist()\n",
    "    x_test = np.asarray(test_df['trillsson'].tolist())\n",
    "    y_test = test_df['class'].tolist()\n",
    "\n",
    "    y_test_names = test_df.file_name.tolist()\n",
    "    y_test_speaker = test_df.speaker.tolist()\n",
    "    y_test_segment_begin = test_df.samples_begin.tolist()\n",
    "    y_test_segment_end = test_df.samples_end.tolist()\n",
    "   \n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65080b1",
   "metadata": {},
   "source": [
    "## Divide into Train and Validation\n",
    "### Parameters:\n",
    "\n",
    "- **df**:  \n",
    "  The original DataFrame containing audio data.\n",
    "\n",
    "- **df_aug**:  \n",
    "  The augmented DataFrame, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of labels for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of labels for the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(df, df_aug, name_aug, random_state=42):\n",
    "\n",
    "    val_list = []\n",
    "    for cls in df['class'].unique():\n",
    "        df_cls = df[df['class'] == cls]\n",
    "        val_sample = df_cls.sample(frac=0.1, random_state=random_state)\n",
    "        val_list.append(val_sample)\n",
    "    val = pd.concat(val_list)\n",
    "    train = df.drop(val.index)\n",
    "\n",
    "    if df_aug is not None:\n",
    "        train_df_aug_key = train.copy()\n",
    "        df_aug = df_aug.copy()\n",
    "        train_df_aug_key['base_file_name'] = train_df_aug_key['file_name']\n",
    "        df_aug['base_file_name'] = df_aug['file_name'].apply(lambda x: x.split('_', 1)[1] if '_' in x else x)\n",
    "        merge_keys = ['base_file_name', 'samples_begin', 'samples_end', 'class']\n",
    "        merged = pd.merge(df_aug, train_df_aug_key[merge_keys], on=merge_keys, how='inner')\n",
    "        train_df = pd.concat([train_df, merged.drop(columns=['base_file_name'])], ignore_index=True)\n",
    "\n",
    "    x_train = np.asarray(train['trillsson'].tolist())\n",
    "    y_train = train['class'].tolist()\n",
    "    x_val = np.asarray(val['trillsson'].tolist())\n",
    "    y_val = val['class'].tolist()\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e3836",
   "metadata": {},
   "source": [
    "## divide intro Train, Validation, Validation for Calibration and Test\n",
    "### Parameters:\n",
    "\n",
    "- **df**:  \n",
    "  The original DataFrame containing audio data.\n",
    "\n",
    "- **df_aug**:  \n",
    "  The augmented DataFrame, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of labels for the training set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of features for the testing set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of labels for the testing set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of labels for the validation set.\n",
    "  \n",
    "- **x_val_cal**:  \n",
    "  Numpy array of features for the validation set for calibration.\n",
    "\n",
    "- **y_val_cal**:  \n",
    "  List of labels for the validation set for calibration.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cal(df, df_aug, name_aug, random_state=42):\n",
    "    test_list = []\n",
    "    val_cal_list = []\n",
    "    train_list = []\n",
    "\n",
    "    val_list = df.sample(frac=0.1, random_state=random_state)\n",
    "    df_remaining = df.drop(val_list.index)\n",
    "\n",
    "    for cls in df_remaining['class'].unique():\n",
    "        df_cls = df_remaining[df_remaining['class'] == cls]\n",
    "        test_sample = df_cls.sample(frac=0.1, random_state=random_state)\n",
    "        remaining = df_cls.drop(test_sample.index)\n",
    "        val_cal_sample = remaining.sample(frac=25/90, random_state=random_state)\n",
    "        train_sample = remaining.drop(val_cal_sample.index)\n",
    "\n",
    "        test_list.append(test_sample)\n",
    "        val_cal_list.append(val_cal_sample)\n",
    "        train_list.append(train_sample)\n",
    "\n",
    "    test_df = pd.concat(test_list)\n",
    "    val_cal_df = pd.concat(val_cal_list)\n",
    "    train_df = pd.concat(train_list)\n",
    "\n",
    "    if df_aug is not None:\n",
    "        train_df_aug_key = train_df.copy()\n",
    "        df_aug = df_aug.copy()\n",
    "        train_df_aug_key['base_file_name'] = train_df_aug_key['file_name']\n",
    "        df_aug['base_file_name'] = df_aug['file_name'].apply(lambda x: x.split('_', 1)[1] if '_' in x else x)\n",
    "        merge_keys = ['base_file_name', 'samples_begin', 'samples_end', 'class']\n",
    "        merged = pd.merge(df_aug, train_df_aug_key[merge_keys], on=merge_keys, how='inner')\n",
    "        train_df = pd.concat([train_df, merged.drop(columns=['base_file_name'])], ignore_index=True)\n",
    "\n",
    "    x_train = np.asarray(train_df['trillsson'].tolist())\n",
    "    y_train = train_df['class'].tolist()\n",
    "    x_val = np.asarray(val_list['trillsson'].tolist())\n",
    "    y_val = val_list['class'].tolist()\n",
    "    x_val_cal = np.asarray(val_cal_df['trillsson'].tolist())\n",
    "    y_val_cal = val_cal_df['class'].tolist()\n",
    "    x_test = np.asarray(test_df['trillsson'].tolist())\n",
    "    y_test = test_df['class'].tolist()\n",
    "\n",
    "    y_test_names = test_df.file_name.tolist()\n",
    "    y_test_speaker = test_df.speaker.tolist()\n",
    "    y_test_segment_begin = test_df.samples_begin.tolist()\n",
    "    y_test_segment_end = test_df.samples_end.tolist()\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_val, y_val, x_val_cal, y_val_cal, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001fb6a7",
   "metadata": {},
   "source": [
    "## get Features for Training, Validation and Test\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of labels for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of labels for the validation set.\n",
    "  \n",
    "- **x_test**:  \n",
    "  Numpy array of features for the testing set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of labels for the testing set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for encoding labels.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **y_train**:  \n",
    "  List of shuffled labels for the training set.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of shuffled features for the training set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of shuffled labels for the validation set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of shuffled features for the validation set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of shuffled labels for the testing set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of shuffled features for the testing set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "  \n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "\n",
    "- **yy_test**:  \n",
    "  Categorical labels for the testing set.\n",
    "\n",
    "- **y_test_names**:  \n",
    "  List of names for the testing set.\n",
    "\n",
    "- **y_test_speaker**:  \n",
    "  List of speakers for the testing set.\n",
    "\n",
    "- **y_test_segment_begin**:  \n",
    "  List of starting sample indices for segments in the testing set.\n",
    "\n",
    "- **y_test_segment_end**:  \n",
    "  List of ending sample indices for segments in the testing set.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5980d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(x_train, y_train, x_val, y_val, x_test, y_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_learn):\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=42)\n",
    "    x_val, y_val = shuffle(x_val, y_val, random_state=42)\n",
    "    if y_test_names is None:\n",
    "        x_test, y_test = shuffle(x_test, y_test, random_state=42)\n",
    "    else:\n",
    "        x_test, y_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end = shuffle(x_test, y_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, random_state=42)\n",
    "\n",
    "    # Encode the classification labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(sorted(df_learn['class'].unique().tolist()))\n",
    "    yy_train = to_categorical(le.transform(y_train))   \n",
    "    yy_val = to_categorical(le.transform(y_val))\n",
    "    yy_test = to_categorical(le.transform(y_test))\n",
    "    \n",
    "    label_mapping = dict(zip(y_train, yy_train))\n",
    "    \n",
    "    return y_train, x_train, y_val, x_val, y_test, x_test, yy_train, yy_val, yy_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0294f",
   "metadata": {},
   "source": [
    "## get Features for Training and Validation\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of labels for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of labels for the validation set.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for encoding labels.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **y_train**:  \n",
    "  List of shuffled labels for the training set.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of shuffled features for the training set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of shuffled labels for the validation set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of shuffled features for the validation set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "\n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19fe1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_finetune_only(x_train, y_train, x_val, y_val, df_learn):\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=42)\n",
    "    x_val, y_val = shuffle(x_val, y_val, random_state=42)\n",
    "\n",
    "    # Encode the classification labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(sorted(df_learn['class'].unique().tolist()))\n",
    "    yy_train = to_categorical(le.transform(y_train))\n",
    "    yy_val = to_categorical(le.transform(y_val))\n",
    "    \n",
    "    label_mapping = dict(zip(y_train, yy_train))\n",
    "    \n",
    "    return y_train, x_train, y_val, x_val, yy_train, yy_val, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0195ed",
   "metadata": {},
   "source": [
    "## get Features for Training, Validation, Calibration Validation and Test\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **y_train**:  \n",
    "  List of labels for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of labels for the validation set.\n",
    "  \n",
    "- **x_val_cal**:  \n",
    "  Numpy array of features for the validation set for Calibration.\n",
    "\n",
    "- **y_val_cal**:  \n",
    "  List of labels for the validation set for Calibration.\n",
    "  \n",
    "- **x_test**:  \n",
    "  Numpy array of features for the testing set.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of labels for the testing set.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for encoding labels.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **y_train**:  \n",
    "  List of shuffled labels for the training set.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of shuffled features for the training set.\n",
    "\n",
    "- **y_val**:  \n",
    "  List of shuffled labels for the validation set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of shuffled features for the validation set.\n",
    "  \n",
    "- **y_val_cal**:  \n",
    "  List of shuffled labels for the validation set for Calibration.\n",
    "\n",
    "- **x_val_cal**:  \n",
    "  Numpy array of shuffled features for the validation set for Calibration.\n",
    "\n",
    "- **y_test**:  \n",
    "  List of shuffled labels for the testing set.\n",
    "\n",
    "- **x_test**:  \n",
    "  Numpy array of shuffled features for the testing set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "\n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "  \n",
    "- **yy_val_cal**:  \n",
    "  Categorical labels for the validation set.\n",
    "  \n",
    "- **yy_test**:  \n",
    "  Categorical labels for the testing set.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5278af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_train_only_cal(x_train, y_train, x_val, y_val, x_val_cal, y_val_cal, x_test, y_test, df_learn):\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=42)\n",
    "    x_val, y_val = shuffle(x_val, y_val, random_state=42)\n",
    "    x_val_cal, y_val_cal = shuffle(x_val_cal, y_val_cal, random_state=42)\n",
    "    x_test, y_test = shuffle(x_test, y_test, random_state=42)\n",
    "\n",
    "    # Encode the classification labels\n",
    "    le = LabelEncoder()\n",
    "    le.fit(sorted(df_learn['class'].unique().tolist()))\n",
    "    yy_train = to_categorical(le.transform(y_train))\n",
    "    yy_val = to_categorical(le.transform(y_val))\n",
    "    yy_val_cal = to_categorical(le.transform(y_val_cal))\n",
    "    yy_test = to_categorical(le.transform(y_test))\n",
    "    \n",
    "    label_mapping = dict(zip(y_train, yy_train))\n",
    "    \n",
    "    return y_train, x_train, y_val, x_val, y_val_cal, x_val_cal, y_test, x_test, yy_train, yy_val, yy_val_cal, yy_test, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81674b6d",
   "metadata": {},
   "source": [
    "## create model\n",
    "### Parameters:\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate for model optimization.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate for regularization.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter for the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter for the dense layers.\n",
    "\n",
    "- **alpha_val**:  \n",
    "  Alpha parameter for LeakyReLU.\n",
    "  \n",
    "- **finetune_only**:  \n",
    "  True if the run is only for finetuning.\n",
    "  \n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **model**:  \n",
    "  Compiled Keras model for classification.\n",
    "\n",
    "- **callback**:  \n",
    "  EarlyStopping callback to monitor validation loss and restore the best weights during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, alpha_val, finetune_only, hyper_test=False):\n",
    "    \n",
    "    seed_value = 42\n",
    "    he_initializer = HeNormal(seed=seed_value)\n",
    "    bias_initializer = Constant(value=0.0)\n",
    "    \n",
    "    classes = df_learn['class'].unique().tolist()\n",
    "    num_labels = len(classes)\n",
    "\n",
    "    METRICS = [\n",
    "        keras.metrics.TruePositives(name='tp'),\n",
    "        keras.metrics.FalsePositives(name='fp'),\n",
    "        keras.metrics.TrueNegatives(name='tn'),\n",
    "        keras.metrics.FalseNegatives(name='fn'), \n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.AUC(name='prc', curve='PR')\n",
    "    ]\n",
    "\n",
    "    def build_model_graph(metrics=METRICS):\n",
    "        model = Sequential()\n",
    "        #model.add(BatchNormalization(input_shape=(np.array(x_train).shape[-1],), name='BatchNorm'))\n",
    "        model.add(LayerNormalization(input_shape=(np.array(x_train).shape[-1],), name='LayerNorm')) \n",
    "        model.add(Dense(units*2,\n",
    "                        kernel_regularizer=l2(l2_val), activity_regularizer=l1(l1_val),\n",
    "                        kernel_initializer=he_initializer, bias_initializer=bias_initializer, name='Dense1'))\n",
    "        model.add(LeakyReLU(alpha=alpha_val))\n",
    "        model.add(Dropout(dr, seed=seed_value, name='Dropout1'))\n",
    "\n",
    "        model.add(Dense(units,\n",
    "                        kernel_regularizer=l2(l2_val), activity_regularizer=l1(l1_val),\n",
    "                        kernel_initializer=he_initializer, bias_initializer=bias_initializer, name='Dense2'))\n",
    "        model.add(LeakyReLU(alpha=alpha_val))\n",
    "        model.add(Dropout(dr, seed=seed_value+1, name='Dropout2'))\n",
    "\n",
    "        model.add(Dense(num_labels, name='Output'))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        if finetune_only:\n",
    "            model.load_weights('model_weights_train.h5')\n",
    "            for layer in model.layers:\n",
    "                layer.trainable = (layer.name == 'Output')\n",
    "                    \n",
    "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(learning_rate = lr))\n",
    "\n",
    "        return model\n",
    "\n",
    "    model = build_model_graph()\n",
    "    #model.save_weights(\"initial_weights.h5\")\n",
    "    if not hyper_test and not finetune_only:\n",
    "        model.load_weights(\"initial_weights.h5\")\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode='min',\n",
    "        min_delta=0.005,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    return model, callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62041397",
   "metadata": {},
   "source": [
    "## train Model\n",
    "### Parameters:\n",
    "\n",
    "- **x_train**:  \n",
    "  Numpy array of features for the training set.\n",
    "\n",
    "- **x_val**:  \n",
    "  Numpy array of features for the validation set.\n",
    "\n",
    "- **yy_train**:  \n",
    "  Categorical labels for the training set.\n",
    "\n",
    "- **yy_val**:  \n",
    "  Categorical labels for the validation set.\n",
    "\n",
    "- **class_weights**:  \n",
    "  Dictionary of class weights for handling class imbalance.\n",
    "\n",
    "- **model**:  \n",
    "  Compiled Keras model for training.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **num_epochs**:  \n",
    "  Number of epochs for training.\n",
    "\n",
    "- **callback**:  \n",
    "  EarlyStopping callback for monitoring validation loss.\n",
    "\n",
    "- **i**:  \n",
    "  Index used for TensorBoard log directory.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the model.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the model.\n",
    "  \n",
    "- **alpha_val**:  \n",
    "  Alpha parameter for LeakyReLU.\n",
    "\n",
    "- **tb**:  \n",
    "  Boolean indicating whether to enable TensorBoard logging.\n",
    "\n",
    "- **log_dir**:  \n",
    "  Directory path for TensorBoard logs.\n",
    "  \n",
    "- **train_only**:  \n",
    "  True if the run is only for training.\n",
    "  \n",
    "- **finetune_only**:  \n",
    "  True if the run is only for finetuning.\n",
    "\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **history**:  \n",
    "  History object containing training metrics and loss values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad8750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(y_train):\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    print('Class weights:', class_weights)\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603170a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, num_epochs, callback, i, lr, dr, units, l1_val, l2_val, alpha_val, tb, log_dir, train_only, finetune_only):\n",
    "      \n",
    "    if tb:\n",
    "        log_dir = log_dir + str(i) + \"lr_\" + str(lr) + \"dr_\" + str(dr) + \"units_\" + str(units) + \"l1_\" + str(l1_val) + \"l2_\" + str(l2_val) + \"alpha_\" + str(alpha_val) + \"bs_\" + str(batch_size)\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "            log_dir=(log_dir), histogram_freq=1,\n",
    "        )\n",
    "        #callbacks = [tensorboard_callback, callback]\n",
    "        callbacks = [tensorboard_callback]\n",
    "    else:\n",
    "        #callbacks = [callback]\n",
    "        callbacks = []\n",
    "    \n",
    "    history = model.fit(np.array(x_train), yy_train, batch_size=batch_size, epochs=num_epochs,\n",
    "                            validation_data=(np.array(x_val), yy_val), verbose=1,\n",
    "                            shuffle=False, class_weight=class_weights, callbacks=callbacks)\n",
    "    \n",
    "    if train_only:\n",
    "        model.save_weights('model_weights_train.h5')\n",
    "        \n",
    "    if finetune_only:\n",
    "        model.save_weights('model_weights_finetune.h5')\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82518459",
   "metadata": {},
   "source": [
    "## test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4f1a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x_test, yy_test, model):\n",
    "    pred_test = model.predict(np.array(x_test))\n",
    "        \n",
    "    classes_x=np.argmax(pred_test,axis=1)\n",
    "    classes_true=np.argmax(yy_test,axis=1)\n",
    "    df_result = pd.DataFrame(list(zip(classes_x, classes_true)), columns=['Pred', 'True'])\n",
    "    \n",
    "    return df_result, classes_x, classes_true, pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9a90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_false(y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_result):\n",
    "    indices = df_result.index[df_result['Pred'] != df_result['True']].tolist()\n",
    "    false_names = [y_test_names[index] for index in indices]\n",
    "    false_speaker = [y_test_speaker[index] for index in indices]\n",
    "    false_segments_begin = [y_test_segment_begin[index] for index in indices]\n",
    "    false_segments_end = [y_test_segment_end[index] for index in indices]\n",
    "    false_simplified = [(y_test_names[index] + ' ' + str(y_test_segment_begin[index]/16000)) for index in indices]\n",
    "    return false_names, false_speaker, false_segments_begin, false_segments_end, false_simplified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af5f9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstPictures(history, num_epochs):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_loss, label='Training loss', color='#185fad')\n",
    "    plt.plot(val_loss, label='Validation loss', color='orange')\n",
    "    plt.title('Training and Validation loss by Epoch', fontsize = 25)\n",
    "    plt.xlabel('Epoch', fontsize = 18)\n",
    "    plt.xticks(range(0,num_epochs,5), range(0,num_epochs,5))\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig('ex_loss_epoch_.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    train_loss = history.history['accuracy']\n",
    "    val_loss = history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(train_loss, label='Training Accuracy', color='#185fad')\n",
    "    plt.plot(val_loss, label='Validation Accuracy', color='orange')\n",
    "    plt.title('Training and Validation Accuracy by Epoch', fontsize = 25)\n",
    "    plt.xlabel('Epoch', fontsize = 18)\n",
    "    plt.xticks(range(0,num_epochs,5), range(0,num_epochs,5))\n",
    "    plt.legend(fontsize = 18)\n",
    "    plt.savefig('ex_acc_epoch.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c6a2d",
   "metadata": {},
   "source": [
    "## main function\n",
    "### Parameters:\n",
    "\n",
    "- **first_pictures**:  \n",
    "  Boolean indicating whether to generate plots for the runs during training.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **df_learn_aug**:  \n",
    "  Augmented DataFrame containing audio data, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "- **i**:  \n",
    "  Index used for TensorBoard log directory.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the dense layers.\n",
    "  \n",
    "- **alpha_val**:  \n",
    "  Alpha parameter for LeakyReLU.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **tb**:  \n",
    "  Boolean indicating whether to enable TensorBoard logging.\n",
    "\n",
    "- **log_dir**:  \n",
    "  Directory path for TensorBoard logs.\n",
    "\n",
    "- **max_epochs**:  \n",
    "  Maximum number of epochs for training.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **list_row**:  \n",
    "  A list containing various metrics and data for evaluation and analysis.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n",
    "\n",
    "### Description:\n",
    "\n",
    "This function orchestrates the entire workflow for training and evaluating the classification model for binary classification. It performs data preprocessing, model creation, training, evaluation, and result extraction. It returns a list of evaluation metrics and data for analysis, along with a mapping of original labels to encoded categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eddc8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all(first_pictures, df_learn, df_learn_aug, name_aug, i, lr, dr, units, l1_val, l2_val, alpha_val, batch_size, tb, log_dir, max_epochs, hyper_test=False):\n",
    "\n",
    "    x_train, y_train, x_test, y_test, x_val, y_val, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end = train_test(df_learn, df_learn_aug, name_aug)\n",
    "    y_train, x_train, y_val, x_val, y_test, x_test, yy_train, yy_val, yy_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, label_mapping = features(x_train, y_train, x_val, y_val, x_test, y_test, y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_learn)\n",
    "    class_weights = weights(y_train)\n",
    "    model, callback = create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, alpha_val, False, hyper_test)\n",
    "    history = train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, max_epochs, callback, i, lr, dr, units, l1_val, l2_val, alpha_val, tb, log_dir, False, False)\n",
    "    df_result, classes_x, classes_true, pred_test = pred(x_test, yy_test, model)\n",
    "    loss, accuracy = model.evaluate(np.array(x_test), yy_test, verbose=0)  \n",
    "    false_names, false_speaker, false_segments_begin, false_segments_end, false_simplified = get_false(y_test_names, y_test_speaker, y_test_segment_begin, y_test_segment_end, df_result)\n",
    "    if first_pictures:\n",
    "        firstPictures(history, max_epochs)\n",
    "    \n",
    "    list_row = [history.history['accuracy'][-1], history.history['val_accuracy'][-1], accuracy,\n",
    "                history.history['loss'][-1], history.history['val_loss'][-1], loss,\n",
    "                false_simplified, classes_x, classes_true, pred_test]\n",
    "    \n",
    "    return list_row, label_mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02442627",
   "metadata": {},
   "source": [
    "## main function for Finetuning\n",
    "### Parameters:\n",
    "\n",
    "- **first_pictures**:  \n",
    "  Boolean indicating whether to generate plots for the runs during training.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **df_learn_aug**:  \n",
    "  Augmented DataFrame containing audio data, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the dense layers.\n",
    "  \n",
    "- **alpha_val**:  \n",
    "  Alpha parameter for LeakyReLU.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **max_epochs**:  \n",
    "  Maximum number of epochs for training.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **list_row**:  \n",
    "  A list containing various metrics and data for evaluation and analysis.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n",
    "\n",
    "### Description:\n",
    "\n",
    "This function orchestrates the entire workflow for finetunng the classification model for binary classification. It performs data preprocessing, model creation, loading model weights, training and saving the model weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78bd1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_finetune(first_pictures, df_learn, df_learn_aug, name_aug, lr, dr, units, l1_val, l2_val, alpha_val, batch_size, max_epochs):\n",
    "\n",
    "    x_train, y_train, x_val, y_val = train_val(df_learn, df_learn_aug, name_aug)\n",
    "    y_train, x_train, y_val, x_val, yy_train, yy_val, label_mapping = features_finetune_only(x_train, y_train, x_val, y_val, df_learn)\n",
    "    class_weights = weights(y_train)\n",
    "    model, callback = create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, alpha_val, True)\n",
    "    history = train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, max_epochs, callback, 0, lr, dr, units, l1_val, l2_val, alpha_val, False, '', False, True)\n",
    "    if first_pictures:\n",
    "        firstPictures(history, max_epochs)\n",
    "    \n",
    "    list_row = [history.history['accuracy'][-1], history.history['val_accuracy'][-1], '',\n",
    "                history.history['loss'][-1], history.history['val_loss'][-1], '',\n",
    "                '', '', '', '']\n",
    "\n",
    "    return list_row, label_mapping\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c958855",
   "metadata": {},
   "source": [
    "## main function for Training\n",
    "### Parameters:\n",
    "\n",
    "- **first_pictures**:  \n",
    "  Boolean indicating whether to generate plots for the runs during training.\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **df_learn_aug**:  \n",
    "  Augmented DataFrame containing audio data, if available.\n",
    "\n",
    "- **name_aug**:  \n",
    "  A name identifier used for loading augmented data or specifying augmentation settings.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the dense layers.\n",
    "  \n",
    "- **alpha_val**:  \n",
    "  Alpha parameter for LeakyReLU.\n",
    "\n",
    "- **batch_size**:  \n",
    "  Batch size used for training.\n",
    "\n",
    "- **max_epochs**:  \n",
    "  Maximum number of epochs for training.\n",
    "  \n",
    "- **calibration**:  \n",
    "  Name of post-hoc probability calibration Method.\n",
    "  \n",
    "### Returns:\n",
    "\n",
    "- **list_row**:  \n",
    "  A list containing various metrics and data for evaluation and analysis.\n",
    "\n",
    "- **label_mapping**:  \n",
    "  Mapping of original dialect labels to encoded categorical labels.\n",
    "\n",
    "### Description:\n",
    "\n",
    "This function orchestrates the entire workflow for training the classification model for binary classification. It performs data preprocessing, model creation, training, calibration and saving the model weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "537daba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(logits, temperature, labels):\n",
    "    scaled_logits = logits / temperature\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=scaled_logits))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7c9da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_train(first_pictures, df_learn, df_learn_aug, name_aug, lr, dr, units, l1_val, l2_val, alpha_val, batch_size, max_epochs, calibration):\n",
    "\n",
    "    if calibration is not None:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val, x_val_cal, y_val_cal, _, _, _, _ = train_cal(df_learn, df_learn_aug, name_aug)\n",
    "        y_train, x_train, y_val, x_val, y_val_cal, x_val_cal, y_test, x_test, yy_train, yy_val, yy_val_cal, yy_test, label_mapping = features_train_only_cal(x_train, y_train, x_val, y_val, x_val_cal, y_val_cal, x_test, y_test, df_learn)\n",
    "    else:\n",
    "        x_train, y_train, x_test, y_test, x_val, y_val, _, _, _, _ = train_test(df_learn, df_learn_aug, name_aug)\n",
    "        y_train, x_train, y_val, x_val, y_test, x_test, yy_train, yy_val, yy_test, _, _, _, _, label_mapping = features(x_train, y_train, x_val, y_val, x_test, y_test, None, None, None, None, df_learn)\n",
    "    class_weights = weights(y_train)\n",
    "    model, callback = create_model(df_learn, x_train, lr, dr, units, l1_val, l2_val, alpha_val, False)\n",
    "    history = train_model(x_train, x_val, yy_train, yy_val, class_weights, model, batch_size, max_epochs, callback, 0, lr, dr, units, l1_val, l2_val, alpha_val, False, '', True, False)\n",
    "    if first_pictures:\n",
    "        firstPictures(history, max_epochs)\n",
    "    \n",
    "    list_row = [history.history['accuracy'][-1], history.history['val_accuracy'][-1], '',\n",
    "                history.history['loss'][-1], history.history['val_loss'][-1], '',\n",
    "                '', '', '', '']\n",
    "\n",
    "    labels_test = np.argmax(yy_test, axis=1)\n",
    "    if calibration is not None:\n",
    "        model_logits = keras.Model(inputs=model.input, outputs=model.get_layer(\"Output\").output)\n",
    "        logits_val = model_logits.predict(x_val_cal, verbose=0)\n",
    "        logits_test = model_logits.predict(x_test, verbose=0)\n",
    "        \n",
    "        if calibration == 'temperature_scaling':\n",
    "            logits = tf.convert_to_tensor(logits_val, dtype=tf.float32)\n",
    "            labels = tf.convert_to_tensor(np.argmax(yy_val_cal, axis=1), dtype=tf.int32)\n",
    "            \n",
    "            with tf.device(\"/CPU:0\"):\n",
    "                temperature = tf.Variable(1.0, dtype=tf.float32)\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "                for _ in range(500):\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        loss = nll_loss(logits, temperature, labels)\n",
    "                    grads = tape.gradient(loss, [temperature])\n",
    "                    optimizer.apply_gradients(zip(grads, [temperature]))\n",
    "            print(f\"Optimized temperature: {temperature.numpy()}\")\n",
    "            \n",
    "            calibrated_probs = tf.nn.softmax(logits_test / temperature).numpy()\n",
    "            labels_temp = np.argmax(yy_test, axis=1)\n",
    "\n",
    "            np.save(\"calibrator_temperature.npy\", temperature.numpy())\n",
    "            reliability_diagram_from_preds(calibrated_probs, labels_temp, filename=\"reliability_diagramm_temperature.png\")\n",
    "            np.savez(\"reliability_data_temperature.npz\", probs=calibrated_probs, labels=labels_temp)\n",
    "            \n",
    "        elif calibration == 'platt':\n",
    "            labels_val = np.argmax(yy_val_cal, axis=1)\n",
    "            binary_logits = logits_val[:, 1].reshape(-1, 1)\n",
    "            platt_model = LogisticRegression(solver='lbfgs')\n",
    "            platt_model.fit(binary_logits, labels_val)\n",
    "\n",
    "            calibrated_probs = platt_model.predict_proba(logits_test[:, 1].reshape(-1, 1))[:, 1]\n",
    "            calibrated_probs = np.stack([1 - calibrated_probs, calibrated_probs], axis=1)\n",
    "            \n",
    "            joblib.dump(platt_model, \"calibrator_platt.pkl\")\n",
    "            reliability_diagram_from_preds(calibrated_probs, labels_test, filename=\"reliability_diagramm_platt.png\")\n",
    "            np.savez(\"reliability_data_platt.npz\", probs=calibrated_probs, labels=labels_test)\n",
    "            \n",
    "        elif calibration == 'isotonic':\n",
    "            labels_val = np.argmax(yy_val_cal, axis=1)\n",
    "            binary_logits = logits_val[:, 1]\n",
    "            iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "            iso_reg.fit(binary_logits, labels_val)\n",
    "\n",
    "            calibrated_probs = iso_reg.predict(logits_test[:, 1])\n",
    "            calibrated_probs = np.stack([1 - calibrated_probs, calibrated_probs], axis=1)\n",
    "\n",
    "            joblib.dump(iso_reg, \"calibrator_isotonic.pkl\")\n",
    "            reliability_diagram_from_preds(calibrated_probs, labels_test, filename=\"reliability_diagramm_isotonic.png\")\n",
    "            np.savez(\"reliability_data_isotonic.npz\", probs=calibrated_probs, labels=labels_test)\n",
    "\n",
    "        elif calibration == 'beta':\n",
    "            probs_val = tf.nn.softmax(logits_val).numpy()\n",
    "            probs_test = tf.nn.softmax(logits_test).numpy()\n",
    "            p_val = probs_val[:, 1]\n",
    "            labels_val = np.argmax(yy_val_cal, axis=1)\n",
    "            \n",
    "            beta_calibrator = BetaCalibration(parameters=\"abm\")\n",
    "            beta_calibrator.fit(p_val, labels_val)\n",
    "\n",
    "            p_test = probs_test[:, 1]\n",
    "            calibrated_probs_pos = beta_calibrator.predict(p_test)\n",
    "            calibrated_probs = np.stack([1 - calibrated_probs_pos, calibrated_probs_pos], axis=1)\n",
    "\n",
    "            joblib.dump(beta_calibrator, \"calibrator_beta.pkl\")\n",
    "            reliability_diagram_from_preds(calibrated_probs, labels_test, filename=\"reliability_diagramm_beta.png\")\n",
    "            np.savez(\"reliability_data_beta.npz\", probs=calibrated_probs, labels=labels_test)\n",
    "            \n",
    "        elif calibration == 'histogram':\n",
    "            probs_val = tf.nn.softmax(logits_val).numpy()\n",
    "            probs_test = tf.nn.softmax(logits_test).numpy()\n",
    "\n",
    "            p_val = probs_val[:, 1]\n",
    "            p_test = probs_test[:, 1]\n",
    "            labels_val = np.argmax(yy_val_cal, axis=1)\n",
    "\n",
    "            n_bins = 20\n",
    "            binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "            bin_ids = binner.fit_transform(p_val.reshape(-1, 1)).astype(int).flatten()\n",
    "\n",
    "            bin_probs = np.zeros(n_bins)\n",
    "            for i in range(n_bins):\n",
    "                in_bin = bin_ids == i\n",
    "                if np.any(in_bin):\n",
    "                    bin_probs[i] = labels_val[in_bin].mean()\n",
    "                else:\n",
    "                    bin_probs[i] = 0.5  # neutral fallback\n",
    "\n",
    "            test_bin_ids = binner.transform(p_test.reshape(-1, 1)).astype(int).flatten()\n",
    "            calibrated_probs_pos = np.array([bin_probs[i] for i in test_bin_ids])\n",
    "            calibrated_probs = np.stack([1 - calibrated_probs_pos, calibrated_probs_pos], axis=1)\n",
    "\n",
    "            joblib.dump((binner, bin_probs), \"calibrator_histogram.pkl\")\n",
    "            reliability_diagram_from_preds(calibrated_probs, labels_test, filename=\"reliability_diagramm_histogram.png\")\n",
    "            np.savez(\"reliability_data_histogram.npz\", probs=calibrated_probs, labels=labels_test)\n",
    "\n",
    "    else:\n",
    "        probs = model.predict(x_test, verbose=0)\n",
    "\n",
    "        reliability_diagram_from_preds(probs, labels_test, filename=\"reliability_diagramm_global_uncalibrated.png\")\n",
    "        np.savez(\"reliability_data_global_uncalibrated.npz\", probs=probs, labels=labels_test)\n",
    "\n",
    "    return list_row, label_mapping\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d22e4b",
   "metadata": {},
   "source": [
    "## main function for Testing\n",
    "### Parameters:\n",
    "\n",
    "- **df_learn**:  \n",
    "  DataFrame containing audio data used for model training.\n",
    "\n",
    "- **df_learn_test**:  \n",
    "  DataFrame containing audio data used for model testing.\n",
    "\n",
    "- **lr**:  \n",
    "  Learning rate used in the model.\n",
    "\n",
    "- **dr**:  \n",
    "  Dropout rate used in the model.\n",
    "\n",
    "- **units**:  \n",
    "  Number of units/neurons in the dense layers of the model.\n",
    "\n",
    "- **l1_val**:  \n",
    "  L1 regularization parameter used in the dense layers.\n",
    "\n",
    "- **l2_val**:  \n",
    "  L2 regularization parameter used in the dense layers.\n",
    "  \n",
    "- **alpha_val**:  \n",
    "  Alpha parameter for LeakyReLU.\n",
    "  \n",
    "- **calibration**:  \n",
    "  Name of post-hoc probability calibration Method.\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **predictions**:  \n",
    "  A list containing all predictions.\n",
    "  \n",
    "- **x_samples_begin**:  \n",
    "  A list containing all beginning indices of segments from predictions.\n",
    "  \n",
    "- **x_samples_end**:  \n",
    "  A list containing all ending indices of segments from predictions.\n",
    "\n",
    "### Description:\n",
    "\n",
    "This function orchestrates the entire workflow for testing the classification model for dialect classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_test(df_learn, df_test, lr, dr, units, l1_val, l2_val, alpha_val, calibration, test_only=True):\n",
    "\n",
    "    x_test = np.asarray(df_test['trillsson'].tolist())\n",
    "    x_samples_begin = np.asarray(df_test['samples_begin'].tolist())\n",
    "    x_samples_end = np.asarray(df_test['samples_end'].tolist())\n",
    "\n",
    "    model, callback = create_model(df_learn, x_test, lr, dr, units, l1_val, l2_val, alpha_val, False)\n",
    "    model.load_weights('model_weights_finetune.h5')\n",
    "\n",
    "    model_logits = keras.Model(inputs=model.input, outputs=model.get_layer(\"Output\").output)\n",
    "    logits_test = model_logits.predict(x_test, verbose=0)\n",
    "\n",
    "    if calibration:\n",
    "        if calibration == 'temperature_scaling':\n",
    "            temperature = np.load(\"calibrator_temperature.npy\")\n",
    "            calibrated_probs = tf.nn.softmax(logits_test / temperature).numpy()\n",
    "\n",
    "        elif calibration == 'platt':\n",
    "            platt_model = joblib.load(\"calibrator_platt.pkl\")\n",
    "            calibrated_probs_pos = platt_model.predict_proba(logits_test[:, 1].reshape(-1, 1))[:, 1]\n",
    "            calibrated_probs = np.stack([1 - calibrated_probs_pos, calibrated_probs_pos], axis=1)\n",
    "\n",
    "        elif calibration == 'isotonic':\n",
    "            iso_reg = joblib.load(\"calibrator_isotonic.pkl\")\n",
    "            calibrated_probs_pos = iso_reg.predict(logits_test[:, 1])\n",
    "            calibrated_probs = np.stack([1 - calibrated_probs_pos, calibrated_probs_pos], axis=1)\n",
    "\n",
    "        elif calibration == 'beta':\n",
    "            beta_calibrator = joblib.load(\"calibrator_beta.pkl\")\n",
    "            calibrated_probs_pos = beta_calibrator.predict(tf.nn.softmax(logits_test)[:, 1])\n",
    "            calibrated_probs = np.stack([1 - calibrated_probs_pos, calibrated_probs_pos], axis=1)\n",
    "            \n",
    "        elif calibration == 'histogram':\n",
    "            binner, bin_probs = joblib.load(\"calibrator_histogram.pkl\")\n",
    "            probs_test = tf.nn.softmax(logits_test).numpy()\n",
    "            p_test = probs_test[:, 1]\n",
    "\n",
    "            test_bin_ids = binner.transform(p_test.reshape(-1, 1)).astype(int).flatten()\n",
    "            calibrated_probs_pos = np.array([bin_probs[i] for i in test_bin_ids])\n",
    "            calibrated_probs = np.stack([1 - calibrated_probs_pos, calibrated_probs_pos], axis=1)\n",
    "\n",
    "        predictions = calibrated_probs\n",
    "    else:\n",
    "        predictions = model.predict(x_test)\n",
    "\n",
    "    return predictions, x_samples_begin, x_samples_end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ef236",
   "metadata": {},
   "source": [
    "## plots the reliability diagram\n",
    "### Parameters:\n",
    "\n",
    "- **pred_proba**:  \n",
    "  List of predicted probabilitys.\n",
    "\n",
    "- **true_labels**:  \n",
    "  List of the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b4ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_diagram_from_preds(pred_proba, true_labels, filename=\"reliability_diagramm_finetuned.png\", n_bins=20):\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    y_proba = pred_proba[:, 1] if pred_proba.shape[1] == 2 else np.max(pred_proba, axis=1)\n",
    "    prob_true, prob_pred = calibration_curve(true_labels, y_proba, n_bins=n_bins, strategy='uniform')\n",
    "\n",
    "    # ECE berechnen\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(y_proba, bins) - 1\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_mask = bin_indices == i\n",
    "        bin_size = np.sum(bin_mask)\n",
    "        if bin_size > 0:\n",
    "            bin_confidence = np.mean(y_proba[bin_mask])\n",
    "            bin_accuracy = np.mean(true_labels[bin_mask])\n",
    "            ece += (bin_size / len(y_proba)) * np.abs(bin_confidence - bin_accuracy)\n",
    "\n",
    "    counts, _ = np.histogram(y_proba, bins=bins)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title('Reliability Diagram')\n",
    "\n",
    "    for center, count in zip(bin_centers, counts):\n",
    "        plt.text(center, 0.05, str(count), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    plt.text(0.95, 0.05, f\"ECE = {ece:.4f}\", ha='right', va='bottom', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06a9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
